\section{More Information about the Formally Verified Checker}
\label{appendix:lean}

Our verified toolchain has been implemented in \lean{}, which is based on
a formal logical foundation in which expressions
have a computational interpretation.
As with other proof assistants like Isabelle and Coq,
a function defined within the formal system can be compiled to efficient code.
At the same time, we can state and prove claims about the function within the system, thereby verifying that the functions compute the intended results.

Our code is contained in the directory {\tt ProofChecker}.
The main checker loop is implemented in {\tt Checker/CheckerCore.lean}.
That file defines a structure {\tt PreState} that contains all the
relevant data structures, which include
the input CNF formula,
the clause database,
the POG under construction,
and the root literal.
We define a {\tt State} of the checker to be a {\tt PreState}
that satisfies all the invariants that we need to establish
the final result,
and the bulk of our work involved showing that
these are indeed preserved by each step of the proof, which
modifies the clause database and the POG.
The ring evaluator and model counter are contained in the
{\tt Count} directory.
Here we provide a few additional notes on the implementation
and clarify the sense in which the toolchain has been verified.

\subsection{Implementation notes}

When thinking about the formal verification, it is helpful to distinguish between
the data structures that play a role in the code that is
executed and the definitions that serve as a mathematical reference,
allowing us to state our specifications and prove that they are met.
Among the former is our representation of CNF formulas:
a literal is represented as a nonzero integer, a clause is an array of literals,
and a CNF formula is an array of clauses.
\begin{lstlisting}
def ILit := { i : Int // i ≠ 0 }
abbrev IClause := Array ILit
abbrev ICnf := Array IClause
\end{lstlisting}
We define the clause database {\tt ClauseDb} as a hashmap that
stores each clause together with a flag indicating whether it has
been deleted.
Each element of a POG is either a variable, a binary disjunction (sum),
or an arbitrary conjunction (product):
\begin{lstlisting}
inductive PogElt where
  | var  : Var → PogElt
  | disj : Var → ILit → ILit → PogElt
  | conj : Var → Array ILit → PogElt
\end{lstlisting}
In the first case, the argument \lstinline{x} in the expression
\lstinline{var x} is the index
of an original variable; in \lstinline{disj x left right} and \lstinline{conj x args}
it is an extension variable appearing in the CPOG file. Note that representing edges as literals
allows us to negate the arguments to \lstinline{disj} and \lstinline{conj}.
A \lstinline{Pog} is then an array of \lstinline{PogElt}
that is well-founded in the sense that each element depends only on prior elements
in the array.

On the mathematical side, we define propositional formulas and
their semantics in the usual way.
Functions \lstinline{ILit.toPropForm}, \lstinline{IClause.toPropForm},
\lstinline{ICnf.toPropForm}, and \lstinline{Pog.toPropForm}
relate our data structures to propositional formulas and their semantics.
For example, given a literal \lstinline{u}, \lstinline{Pog.toPropForm P u}
denotes the (unnegated or negated) interpretation of the node corresponding to \lstinline{u} in the POG
\lstinline{P} as a propositional formula $\phi_\nodeu/\neg\phi_\nodeu$ over the
original variables.
Lean provides us with convenient ``anonymous projector'' notation that allows
us to write \lstinline{P.toPropForm u} instead of \lstinline{Pog.toPropForm P u}
when \lstinline{P} has type \lstinline{Pog},
\lstinline{C.toPropForm} instead of \lstinline{IClause.toPropForm C} when \lstinline{C}
has type \lstinline{IClause}, etc.

In order to reason about the behavior of the checker, we found it important to work
with propositional formulas modulo logical equivalence, a structure known in logic
as the \emph{Lindenbaum--Tarski algebra}.
We defined the quotient and lifted the Boolean operations and the entailment
relation to this new type. The advantage is that equivalent formulas give
rise to equal elements in the quotient.
This makes it much easier to prove equivalences involving complicated expressions,
since it enables us to substitute elements of the quotient for others even when the corresponding
formulas are merely equivalent but not equal. This gave rise to auxiliary challenges, however.
For example, it is no longer straightforward to say that an element of the quotient ``depends'' on a variable,
since equivalent formulas can have different variables---consider $x \vee \neg x$ and $\top$.
Instead, we made use of a semantic notion of dependence, in which an element $\phi$ of the
quotient depends on a variable if and only if for some truth assignment the value of $\phi$
changes when the assignment to that variable is flipped.

\subsection{Trust}

In this subsection, we clarify what has been verified and what has to be trusted.
Recall that our first step is to parse CNF and CPOG files in order to read in the initial
formula and the proof. We do not verify this step. Instead, the verified checker exposes
flags \verb+--print-cnf+ and \verb+--print-cpog+ which reprint the consumed formula or proof,
respectively. Comparing this to the actual files using {\tt diff} provides an easy way of ensuring
that what was parsed matches their contents. This involves trusting only the correctness of the
print procedure and {\tt diff}. Similarly, if one wants to establish the correctness of the POG
contained in the CPOG file, one can print out the POG that is constructed by the checker and compare.

Lean's code extraction replaces calculations on natural numbers and integers with
efficient but unverified arbitrary precision versions.
Lean also uses an efficient implementation of arrays; within the
formal system, these are defined in terms of lists, but code extraction replaces them
with dynamic arrays and uses reference counting to allow destructive updates when it is safe
to do so \cite{Ullrich:de:Moura:19}.
Finally, Lean's standard library implements hashmaps in terms of arrays.
Many of the basic properties of hashmaps have been formally verified, but not all.
In particular, we make use of a fold operation whose verification is not yet complete.
Thus our proofs depend on the assumption that the fold operation has the
expected properties.

In summary, in addition to trusting Lean's foundation and kernel checker,
we also have to trust that code extraction respects that foundation,
that the implementation of the fold operation for hashmaps satisfies its description,
and that, after parsing, the computation uses the correct input formula.
All of our specifications have been completely proven and verified relative to these assumptions.

