\section{More Information about the Formally Verified Checker}
\label{appendix:lean}

Our verified toolchain has been implemented in \lean{}, which is based on
a formal logical foundation in which expressions
have a computational interpretation.
As with other proof assistants like Isabelle and Coq,
a function defined within the formal system can be compiled to efficient code.
At the same time, we can state and prove claims about the function within the system, thereby verifying that the functions compute the intended results.

Our code is contained in the directory {\tt ProofChecker}.
The main checker loop is implemented in {\tt Checker/CheckerCore.lean}.
That file defines a structure {\tt PreState} that contains all the
relevant data structures, which include
the input CNF formula,
the clause database,
the POG under construction,
and the root literal.
We define a {\tt State} of the checker to be a {\tt PreState}
that satisfies all the invariants that we need to establish
the final result,
and the bulk of our work involved showing that
these are indeed preserved by each step of the proof, which
modifies the clause database and the POG.
The ring evaluator and model counter are contained in the
{\tt Count} directory.
Here we provide a few additional notes on the implementation
and clarify the sense in which the toolchain has been verified.

\subsection{Implementation notes}

When thinking about the formal verification, it is helpful to distinguish between
the data structures that play a role in the code that is
executed and the definitions that serve as a mathematical reference,
allowing us to state our specifications and prove that they are met.
Our representation of CNF formulas is among the former:
a literal is represented as a nonzero integer, a clause is an array of literals,
and a CNF formula is an array of clauses.
\begin{lstlisting}
def ILit := { i : Int // i ≠ 0 }
abbrev IClause := Array ILit
abbrev ICnf := Array IClause
\end{lstlisting}
We define the clause database {\tt ClauseDb} as a hashmap that
stores each clause together with a flag indicating whether it has
been deleted.
Each element of a POG is either a variable, a binary disjunction (sum),
or an arbitrary conjunction (product):
\begin{lstlisting}
inductive PogElt where
  | var  : Var → PogElt
  | disj : Var → ILit → ILit → PogElt
  | conj : Var → Array ILit → PogElt
\end{lstlisting}
In the first case, the argument \lstinline{x} in the expression
\lstinline{var x} is the index
of the variable; in \lstinline{disj x left right} and \lstinline{conj x args}
it is the definition number in the CRAT file. Note that representing edges as literals allows us to negate the arguments to \lstinline{disj} and \lstinline{conj}.
A \lstinline{Pog} is then an array of \lstinline{PogElt}
that is well founded in the sense that each element depends only on prior elements
in the array.

On the mathematical side, we define propositional formulas and
their semantics in the usual way.
Functions \lstinline{ILit.toPropForm}, \lstinline{IClause.toPropForm},
\lstinline{ICnf.toPropForm}, and \lstinline{Pog.toPropForm}
relate our data structures to propositional formulas and their semantics.
For example, given a literal \lstinline{l}, \lstinline{Pog.toPropForm P l}
denotes the (unnegated or negated) interpretation of the node of the POG
\lstinline{P} specified by \lstinline{l} as a propositional formula over the
original variables.
Lean provides us with convenient ``anonymous projector'' notation that allows
us to write \lstinline{P.toPropForm l} instead of \lstinline{Pog.toPropForm P l}
when \lstinline{P} has type \lstinline{Pog},
\lstinline{C.toPropForm} instead of \lstinline{IClause.toPropForm C} when \lstinline{C}
has type \lstinline{IClause}, etc.

In order to reason about the behavior of the checker, we found it important to work
with propositional formulas modulo logical equivalence, a structure known in logic
as the \emph{Lindenbaum--Tarski algebra}.
We defined the quotient and lifted the boolean operations and the entailment
relation to this new type. The advantage is that equivalent formulas give
rise to equal elements in the quotient, which
made is much easier to prove equivalences involving complicated expressions,
since it enabled us to calculate with the associated
elements of the quotient and substitute one for another when the corresponding formulas are equivalent. This gave rise to auxiliary challenges, however.
For example, it is no longer straightforward to say that an element of the quotient ``depends'' on a variable,
since equivalent formulas can have difference variables.
Instead, we made use of a semantic notion of dependence, in which an element of the
quotient depends on a variable if and only if for some truth assignment its value
changes when the assignment to that variable is flipped.

\subsection{Trust}

In this subsection, we clarify what has been verified and what has been trusted.
Recall that our first step is to parse a CRAT file to read in the initial CNF formula
and the CRAT proof. We do not verify this step.
If one is only interested in obtaining a model count or a weighted model count,
there is no strong need to verify the parsing of the proof,
because if the checker succeeds in constructing an equivalent POG,
it does not really matter whether the proof was the one the user intended.
But it would be nice to have reassurance that the CNF formula that serves
as input to our verified checker matches the one described in the file.
An easy and standard way of addressing this is to print out the CNF formula
after parsing it and then do a {\tt diff} with the original input.
This involves trusting only the correctness of the print procedure and {\tt diff}.
Similarly, if one wants to establish the correctness of the POG contained in the CRAT file,
one can print out the POG that is constructed by the checker and compare.

Lean's code extraction replaces calculations with natural numbers and integers with
efficient but unverified arbitrary precision versions.
Lean also uses an efficient implementation of arrays; within the
formal system, these are defined in terms of lists, but code extraction replaces them
with dynamic arrays and uses reference counting to allow destructive updates when it is safe
to do so \cite{Ullrich:de:Moura:19}.
Finally, Lean's standard library implements hashmaps in terms of arrays.
Many of the basic properties of hashmaps have been formally verified, but not all.
In particular, we make use of a fold operation whose verification is not yet complete.
Thus our proofs depend on the assumption that the fold operation has the
expected properties.

In sum, in addition to trusting Lean's foundation and kernel checker,
we also have to trust that code extraction respects that foundation,
that the implementation of arrays and the fold operation for hashmaps
satisfy their internal descriptions,
and that, after parsing, the computation has the correct input formula.
All of our specifications have been completely proved and verified relative to that.

