\documentclass[twoside,11pt]{article}
\usepackage{jair, theapa, rawfonts}

\jairheading{1}{2024}{1--16}{10/22}{03/24}

\ShortHeadings{Certified Knowledge Compilation}{Bryant, Nawrocki, Avigad, \& Heule}

\usepackage{amsmath}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{booktabs}
\usepackage{hyperref}

\newcommand{\pand}{\mathbin{\land^{\sf p}}}
\newcommand{\por}{\mathbin{\lor^{\sf p}}}
\DeclareMathOperator*{\Pand}{\bigwedge^{\sf v}}
\DeclareMathOperator*{\Por}{\bigvee^{\sf a}}
\newcommand{\boolnot}{\neg}
\newcommand{\tautology}{\top}
\newcommand{\nil}{\bot}
\newcommand{\obar}[1]{\overline{#1}}
\newcommand{\oneminus}{{\sim}}
\newcommand{\lit}{\ell}

\newcommand{\varset}{X}
\newcommand{\exvarset}{Z}
\newcommand{\dependencyset}{{\cal D}}
\newcommand{\litset}{{\cal L}}
\newcommand{\ring}{{\cal R}}
\newcommand{\dset}{{\cal A}}
\newcommand{\rep}{\textbf{R}}
\newcommand{\radd}{+}
\newcommand{\rmul}{\times}
\newcommand{\addident}{\textbf{0}}
\newcommand{\mulident}{\textbf{1}}
\newcommand{\imply}{\Rightarrow}

\newcommand{\assign}{\alpha}
\newcommand{\passign}{\rho}
\newcommand{\uassign}{{\cal U}}
\newcommand{\modelset}{{\cal M}}

\newcommand{\indegree}{\textrm{indegree}}
\newcommand{\outdegree}{\textrm{outdegree}}
\newcommand{\validate}{\textsf{validate}}

\newcommand{\makenode}[1]{\mathbf{#1}}
\newcommand{\nodeu}{\makenode{u}}
\newcommand{\nodev}{\makenode{v}}
\newcommand{\nodes}{\makenode{s}}
\newcommand{\nodep}{\makenode{p}}
\newcommand{\noder}{\makenode{r}}

\newcommand{\simplify}[2]{#1|_{#2}}
\newcommand{\prov}{\mathit{Prov}}

\bibliographystyle{theapa}

\title{Certified Knowledge Compilation \\ with Application to Verified Model Counting \\ Experimental Details }

\author{\name Randal E. Bryant \email rebryant@cmu.edu \\
  \name Wojciech Nawrocki \email wjnawrock@cmu.edu \\
  \name Jeremy Avigad \email avigad@cmu.edu \\
  \name Marijn J. H. Heule \email marijn@cmu.edu \\
  \addr Carnegie Mellon University \\ Pittsburgh, PA  USA}

\newcommand{\program}[1]{\textsc{#1}}
\newcommand{\lean}{\program{lean4}}

\begin{document}

\maketitle

\definecolor{redorange}{rgb}{0.878431, 0.235294, 0.192157}
\definecolor{lightblue}{rgb}{0.552941, 0.72549, 0.792157}
\definecolor{clearyellow}{rgb}{0.964706, 0.745098, 0}
\definecolor{clearorange}{rgb}{0.917647, 0.462745, 0}
\definecolor{mildgray}{rgb}{0.54902, 0.509804, 0.47451}
\definecolor{softblue}{rgb}{0.643137, 0.858824, 0.909804}
\definecolor{bluegray}{rgb}{0.141176, 0.313725, 0.603922}
\definecolor{lightgreen}{rgb}{0.709804, 0.741176, 0}
\definecolor{darkgreen}{rgb}{0.152941, 0.576471, 0.172549}
\definecolor{redpurple}{rgb}{0.835294, 0, 0.196078}
\definecolor{midblue}{rgb}{0, 0.592157, 0.662745}
\definecolor{clearpurple}{rgb}{0.67451, 0.0784314, 0.352941}
\definecolor{browngreen}{rgb}{0.333333, 0.313725, 0.145098}
\definecolor{darkestpurple}{rgb}{0.396078, 0.113725, 0.196078}
\definecolor{greypurple}{rgb}{0.294118, 0.219608, 0.298039}
\definecolor{darkturquoise}{rgb}{0, 0.239216, 0.298039}
\definecolor{darkbrown}{rgb}{0.305882, 0.211765, 0.160784}
\definecolor{midgreen}{rgb}{0.560784, 0.6, 0.243137}
\definecolor{darkred}{rgb}{0.576471, 0.152941, 0.172549}
\definecolor{darkpurple}{rgb}{0.313725, 0.027451, 0.470588}
\definecolor{darkestblue}{rgb}{0, 0.156863, 0.333333}
\definecolor{lightpurple}{rgb}{0.776471, 0.690196, 0.737255}
\definecolor{softgreen}{rgb}{0.733333, 0.772549, 0.572549}
\definecolor{offwhite}{rgb}{0.839216, 0.823529, 0.768627}
\definecolor{medgreen}{rgb}{0.15, 0.6, 0.15}

\begin{abstract}
  This document gives details on the experiments
\end{abstract}
  

\section*{Benchmark machine}
\begin{itemize}
\item 2021 MacBook Pro 18,4
  \begin{itemize}
  \item 3.2 Ghz Apple M1 processor
  \item 64 GB Ram
  \end{itemize}
\item  Samsung T7 solid-state disk
  \begin{itemize}
  \item 2TB capacity
  \item Up to 1 GB/s R/W
  \item Critical for performance.  Wrote and read files of over 162 GB.
  \end{itemize}
\end{itemize}

\section*{Methodology}
\begin{enumerate}
   \item Retrieved 200 public model counting benchmarks from 2022 Model Counting Competition website
     \begin{center}
       \url{https://mccompetition.org/2022/mc_description.html}
     \end{center}
     \begin{itemize}
     \item 100 public instances from Track 1 (unweighted model counting)
     \item 100 public instances from Track 2 (weighted model counting)
     \end{itemize}
   \item Removed duplicates
     \begin{itemize}
    \item  20 instances were duplicated in the two tracks.  
     \item Reduced set of benchmarks to 180 by including some from Track 1 and some from Track 2
     \end{itemize}
   \item Ran D4 on 180 instances with time limit of 4000 seconds
     \begin{itemize}
     \item Completed for 123 instances
     \end{itemize}
   \item Generated POG representations from dDNNF graphs
     \begin{itemize}
     \item Used heuristics to identify root node, and then put in topological order with root node at end.
     \item Successfully generated POG representation for 122 benchmarks
       \begin{itemize}
         \item One failed with too many defining clauses (2,761,457,765).  That count exceeded the 32-bit signed integer used to represent clause identifiers.
       \end{itemize}
       
     \item Measure complexity by number of defining clauses $D$
       \begin{itemize}
       \item Product or sum node with degree $k$ requires $k+1$ clauses
       \item $D$ also equals sum of number of edges and number of nodes
       \item For 123 instances, Min = 304, Median = 774,883, Mean = 72,389,514, Max = 2,761,457,765 (1,765,743,261 for those that didn't overflow).
       \end{itemize}

     \item Also measure \emph{tree ratio}: $D_t / D$.
       \begin{itemize}
       \item $D_t$ Number of defining clauses if graph were expanded to have no shared subgraphs (i.e., a tree)
       \item $D$ Number of defining clauses for the POG representation
       \item Provides an indication of the importance of exploiting shared subgraphs in the POG generation
       \end{itemize}

     \item Performance (122 instances.  Don't include the one that overflowed):
       \begin{itemize}
       \item Times: Min = 0.012 secs, Max = 2804.5 secs, Median = 3.46 secs
       \item Defining clauses: Min = 304, Max = 1,765,743,261, Median = 774,883
       \item Tree Sizes ($D_t$): Min = 304, max = 14,924,670,455,484, median = 7,669,582.
       \item Tree ratio: Min = 1.0, Max = 52,409.7, Median = 11.61, Harmonic Mean = 3.21
       \end{itemize}
     \end{itemize}
\end{enumerate}

\section*{Selecting Reduced Benchmark Set}
       
To make the experiments tractable, want to select set of benchmarks that are more tractable.
Limit set to the 90 with at most $10^7$ defining clauses, and for which D4 runs in at most 1000 seconds.

D4 Performance (90 instances):
       \begin{itemize}
       \item Times: Min = 0.012 secs, Max = 660.2 secs, Median = 1.72 secs
       \item Defining clauses: Min = 304, Max = 8,493,275, Median = 378,325
       \item Tree Sizes ($D_t$): Min = 304, max = 1,205,743,897, median = 2,690,776
       \item Tree ratio: Min = 1.0, Max = 363.9, Median = 9.98, Harmonic Mean = 2.94
       \end{itemize}

\section*{CPOG Generation}

We have devised two different approaches to generating the forward implication proof from the dec-DNNF generated by {\sc D4}:
\begin{description}
\item[Monolithic:] Make a single call to a proof generating SAT solver.
\item[Structured:] Traverse the POG top-down and recursively generate a forward implication proof for each node encountered.  Lemmas can be introduced to avoid an exponential expansion of the POG into a tree.
\end{description}

Our implementation also allows a hybrid of these approaches: at any
point in the structured traversal, it can generate a proof for a node via a
single call to a SAT solver.  This capability supports both full
monolithic generation (perform monolithic generation at the POG root),
full structured generation (continue traversing to the POG leaves), or
a balance between the two.
The challenge is then to devise a
heuristic criterion for deciding whether to continue the traversal below
some node or to generate a proof for that node monolithically.

\begin{figure}
\centering{%
\begin{tikzpicture}[scale = 0.70]
  \begin{axis}[mark options={scale=0.55},grid=both, grid style={black!10}, ymode=log,
      legend style={at={(0.30,0.96)}},
      legend cell align={left},
                              x post scale=2.0, y post scale=2.0,
                              xmode=log,xmin=0.01,xmax=1800,
                              xtick={0.01, 0.1,1.0,10,100,1000}, xticklabels={0.01, 0.1, 1.0, 10, 100, {1,000}},
                              ymode=log, ymin=0.01, ymax=1800,
                              ytick={0.01, 0.1,1.0,10,100,1000}, yticklabels={0.01, 0.1, 1.0, 10, 100, {1,000}},
                              xlabel={Monolithic generation + check (seconds)}, ylabel={Structural generation + check (seconds)},
            ]

    \input{data-formatted/sub10M-seconds-subthresh}
    \input{data-formatted/sub10M-seconds-supthresh}
    \legend{
      \scriptsize \textsf{Tree ratio $\leq \; 5{.}0$},
      \scriptsize \textsf{Tree ratio $> \; 5{.}0$},
    }
    \addplot[mark=none, color=lightblue] coordinates{(0.01,0.01) (1000,1000)};
    \addplot[mark=none, color=lightblue] coordinates{(0.01,1000) (1000,1000)};
    \addplot[mark=none, color=lightblue] coordinates{(1000,0.01) (1000,1000)};
    \addplot[color=black,dashed] coordinates{(1e-2,1500) (1500,1500)};
    \addplot[color=black,dashed] coordinates{(1500,1e-2) (1500,1500)};

          \end{axis}
\end{tikzpicture}
} % centering
\caption{Structural (Y-axis) versus monolithic (X-axis) execution times.  Monolithic generally performed better for lower tree ratios.}
\label{fig:seconds:mono:structured}
\end{figure}

Our first set of experiments applies full monolithic and full
structured generation to the reduced benchmark set.  Figure
~\ref{fig:seconds:mono:structured} shows a plot comparing the two
approaches.  Each axis shows the number of seconds to generate and
check a CPOG file.  Data points to the left of the diagonal line ran
faster with the structural method, while those to the right ran faster
with the monolithic method.  Five of the benchmarks exceeded the 1000-second threshold for both approaches.
The data are divided into two groups:
those having tree ratios below 5.0 and those having tree ratios above
5.0.  
As can be seen there is some correlation.  Of the 85 benchmarks for which at least one completed within 1000 seconds:
\begin{itemize}
\item For those with tree ratios below 5.0, 12 ran faster with monolithic generation and 23 with structural
\item For those with tree ratios above 5.0, 37 ran faster with monolithic generation and 13 with structural
\end{itemize}
Based on this, we devised the following selection rule: when the
tree ratio is at most $5.0$, use the monolithic approach, otherwise
use the structured approach.  That would yield the better choice for
60 of the 85 cases.  In terms of failures, there is one benchmark with
a high tree ratio for which the monolithic approach requires 835
seconds, and the structural approach requires 1388. This is therefore
marked as a failure by following our rule.  On the other hand, the rule makes the better choice for four of the cases where one approach times out while the other completes.
Our data set was too sparse to do more tuning, including the optimal threshold selection.

Overall, our results on the 90 benchmarks were:
\begin{itemize}
\item Monolithic: 83 completed
\item Structured: 82 completed
\item Threshold of 5.0: 85 completed
\end{itemize}

We tried various experiments where the proof generator starts at the
top using a structural approach and then switches to a monolithic
approach once the tree size for a node drops below some threshold.
This was helpful for very large benchmarks, using high thresholds.
We also found that SAT solvers could not reliably handle problems with $10^7$ or more clauses.
We therefore refined the rule into a \emph{hybrid} approach that operates as follows:
\begin{enumerate}
\item With a bottom-up traversal of the graph, Label each node by the tree size of the subgraph for which it is the root.
\item Compute the total size of the graph and the tree ratio of the root.
\item Proceed with proof generation with the following rules
  \begin{enumerate}
  \item If the tree ratio is below 5.0 and the tree size of the root is below $10^6$, do the entire proof generation with a monolithic approach
  \item If the tree ratio is below 5.0 and the tree size of the root is above $10^6$, start with a structural approach and shift to a monolithic approach once the tree size at a node is below $10^6$.
  \item If the tree ratio is above 5.0, do the entire proof generation
    with a structural approach.
  \end{enumerate}
\end{enumerate}
Unless noted otherwise, The remainder of our experimental data is based on this approach.

\begin{figure}
\centering{%
\begin{tikzpicture}[scale = 0.70]
  \begin{axis}[mark options={scale=0.55},grid=both, grid style={black!10}, ymode=log,
      legend style={at={(0.30,0.96)}},
      legend cell align={left},
                              x post scale=2.0, y post scale=2.0,
                              xmode=log,xmin=1e2,xmax=3e7,
                              xtick={100,1000, 10000, 100000, 1000000, 10000000, 100000000, 1000000000, 1e10}, 
                              xticklabels={$10^2$,$10^3$,$10^4$,$10^5$,$10^6$,$10^7$,$10^8$,$10^9$, $10^{10}$},
                              ymode=log,ymin=1e2,ymax=3e7,
                              ytick={100,1000, 10000, 100000, 1000000, 10000000, 100000000, 1000000000, 1e10}, 
                              yticklabels={$10^2$,$10^3$,$10^4$,$10^5$,$10^6$,$10^7$,$10^8$,$10^9$, $10^{10}$},
                              xlabel={Monolithic proof (clauses)}, ylabel={Structural proof  (clauses)},
            ]

    \input{data-formatted/sub10M-clauses-subthresh}
    \input{data-formatted/sub10M-clauses-supthresh}
    \legend{
      \scriptsize \textsf{Tree ratio $\leq \; 5{.}0$},
      \scriptsize \textsf{Tree ratio $> \; 5{.}0$},
    }
    \addplot[mark=none, color=lightblue] coordinates{(100,100) (2e7,2e7)};
    \addplot[mark=none, color=lightblue] coordinates{(100,2e7) (2e7,2e7)};
    \addplot[mark=none, color=lightblue] coordinates{(2e7,100) (2e7,2e7)};
    \addplot[color=black,dashed] coordinates{(100,2.5e7) (2.5e7,2.5e7)};
    \addplot[color=black,dashed] coordinates{(2.5e7,100) (2.5e7,2.5e7)};

          \end{axis}
\end{tikzpicture}
} % centering
\caption{Structural (Y-axis) versus monolithic (X-axis) proof sizes.  Monolithic generally performed better for lower tree ratios.}
\label{fig:clauses:mono:structured}
\end{figure}

Figure~\ref{fig:clauses:mono:structured} shows the comparative proof
sizes (in clauses) for the two approaches.  As can be seen, the
monolithic approach tends to generate shorter proofs.  Of the 80 cases
for which both methods completed within 1000 seconds, 18 had shorter
proofs with the structural approach and 62 with the monolithic
approach.  In setting our tree ratio threshold to 5.0, we get the shorter proofs for 45 cases and suboptimal proof sizes for 40.

\section*{Summary Result}


Our summary data is based on the hybrid approach.

Summary:

\begin{itemize}
\item Of the 123 benchmarks, 110 were completed using the hybrid approach within 10,000 seconds.
\item One additional one completed within 10,000 for structural (and for hybrid with a threshold of $10^5$ clauses).
\item For seven others, was able to do one-sided proof
\item Only one had more than $10^9$ defining clauses.
\item Total of 5 (including 1 overflow case) for which no form of validation succeeded.
  \begin{itemize}
  \item Three of these had more than $10^9$ defining clauses
  \item Smallest had 405 million
  
  \end{itemize}
\end{itemize}

\begin{figure}
\centering{%
\begin{tikzpicture}[scale = 0.70]
  \begin{axis}[mark options={scale=0.55},grid=both, grid style={black!10}, ymode=log,
      legend style={at={(0.30,0.96)}},
      legend cell align={left},
                              x post scale=1.8, y post scale=2.4,
                              xmode=log,
                              xmin=0.01,xmax=4000,
                              xtick={0.01, 0.1,1.0,10,100,1000,10000}, xticklabels={0.01, 0.1, 1.0, 10, 100, {1,000}, {10,000}},
                              ymin=0.01, ymax=29000,
                              ytick={0.01, 0.1,1.0,10,100,1000,10000,100000}, yticklabels={0.01, 0.1, 1.0, 10, 100, {1,000},{10,000},{100,000}},
                              xlabel={D4 runtime (seconds)}, ylabel={CPOG generation and checking runtime (seconds)},
%                              title={D4 Defining Clause Generation}
            ]
    \input{data-formatted/seconds-summary}
    \input{data-formatted/seconds-summary-structured-only}
    \input{data-formatted/seconds-summary-onesided}
    \input{data-formatted/seconds-summary-failures}
    \legend{
      \scriptsize \textsf{Full validation, Hybrid},
      \scriptsize \textsf{Full validation, Structural},
      \scriptsize \textsf{One-sided validation},
      \scriptsize \textsf{Resource limit exceeded},
    }
    \addplot[mark=none, dashed] coordinates{(0.01,25000) (4000, 25000)};) 
    \addplot[mark=none, color=lightblue] coordinates{(0.01,20000) (4000,20000)};
    \addplot[mark=none, color=lightblue] coordinates{(0.1,0.01) (10000.0,1000.0)};
    \addplot[mark=none, color=lightblue] coordinates{(0.01,0.01) (10000.0,10000.0)};
    \addplot[mark=none, color=lightblue] coordinates{(0.01,0.1) (2000,20000)};
    \addplot[mark=none, color=lightblue] coordinates{(0.01,1.0) (200, 20000)};
    \addplot[mark=none, color=lightblue] coordinates{(0.01,10.0) (20, 20000)};
    \node[left] at (axis cs: 0.9,0.12) {$0{.}1\times$};
    \node[left] at (axis cs: 0.09,0.12) {$1\times$};
    \node[left] at (axis cs: 0.09,1.2) {$10\times$};
    \node[left] at (axis cs: 0.09,12.0) {$100\times$};
    \node[left] at (axis cs: 0.09,120.0) {$1000\times$};
       \end{axis}

\end{tikzpicture}
} % centering
\caption{Combined runtime for CPOG proof generation and checking  as function of D4 runtime.  Timeouts are shown as points on the dashed line.}
\label{fig:d4:cpog}
\end{figure}

Figure~\ref{fig:d4:cpog} compares the running time for the combination
of generating and checking the CPOG file (Y axis), versus the running
time for C4 (X Axis).  If we compare the ratio between the two we find the following:
\begin{itemize}
\item Min = 0.27 (i.e., verification chain was $3.64\times$ faster)
\item Max = 177.0
\item Median = 12.5
\end{itemize}

\begin{figure}
\centering{%
\begin{tikzpicture}[scale = 0.70]
  \begin{axis}[mark options={scale=0.55},grid=both, grid style={black!10},
      legend style={at={(0.92,0.28)}},
      legend cell align={left},
                              x post scale=1.9, y post scale=2.10,
                              xmode=log,xmin=100,xmax=1e10, 
                              xtick={100,1000, 10000, 100000, 1000000, 10000000, 100000000, 1000000000, 1e10}, 
                              xticklabels={$10^2$,$10^3$,$10^4$,$10^5$,$10^6$,$10^7$,$10^8$,$10^9$, $10^{10}$},
                              ymode=log, ymin=100, ymax=1800000000, 
                              ytick={100,1000, 10000, 100000, 1000000, 10000000, 100000000, 1000000000},
                              yticklabels={$10^2$,$10^3$,$10^4$,$10^5$,$10^6$,$10^7$,$10^8$,$10^9$},
                              xlabel={Defining Clauses}, ylabel={Proof Clauses},
            ]
    \input{data-formatted/clauses-summary}
    \input{data-formatted/clauses-summary-structured-only}
    \input{data-formatted/clauses-summary-onesided}
    \input{data-formatted/clauses-summary-failures}

    \legend{
      \scriptsize \textsf{Full validation, Hybrid},
      \scriptsize \textsf{Full validation, Structural},
      \scriptsize \textsf{One-sided validation},
      \scriptsize \textsf{No validation},
    }
    \addplot[mark=none, color=lightblue] coordinates{(100,100) (1e9,1e9)};
    \addplot[mark=none, color=lightblue] coordinates{(100,1000) (1e8,1e9)};
    \addplot[mark=none, color=lightblue] coordinates{(100,10000) (1e7,1e9)};
    \addplot[mark=none, color=lightblue] coordinates{(100,100000) (1e6,1e9)};
    \addplot[mark=none, color=lightblue] coordinates{(100,1000000) (1e5,1e9)};
    \addplot[mark=none, color=lightblue] coordinates{(100,1e9) (1e10,1e9)};
    \addplot[color=black,dashed] coordinates{(1e2,1.5e9) (1e10,1.5e9)};
    \node[left] at (axis cs: 900,1200) {$1\times$};
    \node[left] at (axis cs: 900,12000) {$10\times$};
    \node[left] at (axis cs: 900,120000) {$100\times$};
    \node[left] at (axis cs: 900,1200000) {$1{,}000\times$};
    \node[left] at (axis cs: 900,12000000) {$10{,}000\times$};

          \end{axis}
\end{tikzpicture}
} % centering
\caption{Total number of clauses in CPOG file as function of number of defining clauses}
\label{fig:defining:total}
\end{figure}

Figure~\ref{fig:defining:total} compares the total number of clauses in the CPOG representation (Y axis) versus the number of defining clauses (X axis).  Since the former include the latter, the ratio between these cannot be less than 1.0.
Comparing the ratios gives:
\begin{itemize}
\item Min = 1.02
\item Max = 9460.2
\item Median = 2.29
\end{itemize}


\end{document}


\section*{Benefits of Optimizations}

Two optimizations were implemented within the CPOG framework:
\begin{description}
\item[Lemmas:] Nodes with indegree greater than one were validated with separate lemmas, avoiding the possibly exponential expansion of the POG into a tree during the proof construction.

\item[Literal grouping:] For a product node with multiple literal
  arguments, those that cannot be validated by BCP are combined as
  arguments to a newly defined product node.  The extension variable
  associated with this node is validated, and the proof is then used
  to validate the literals.  This reduces the number of
  calls to the SAT solver to at most one per product node.
\end{description}
These optimizations are only applicable when the forward implication
proof is generated by the structural method, and so we concentrate on
the 50 benchmarks (from the smaller set of 90 benchmarks having at
most $10^7$ defining clauses) having tree ratios greater than $5.0$.

\input{figure-clauses-optimizations}


We conducted a set of experiments using the 80 benchmark instances for
which the total number of proof clauses when both optimizations are
performed was limited to $10^7$.  We then made three additional runs
of the CPOG generator and checker, with each of the two optimizations
disabled individually and in combination.  Figures
\ref{fig:optimized:partially-optimized}--\ref{fig:optimized:lessoptimized} illustrates the effect of the two
optimizations.  Each set of points shows the number of clauses when
both optimizations are performed for the $X$ value and the number with
one or both of these optimizations disabled as the $Y$ value.

The ratios between the number of clauses when either or both optimizations is disabled versus when both are enabled can be summarized as:
\begin{description}
\item[Without lemmas:] Min 1.00, Max 52.54, Harmonic mean 2.11
\item[Without literal grouping:] Min 0.84, Max 39.60, Harmonic mean 1.18
\item[Without either optimization:] Min 0.99, Max 52.54, Harmonic mean 2.95
\end{description}

Here are other ways to summarize the impact on proof length:
\begin{itemize}
\item None of the 80 benchmarks require more than $10^7$
proof clauses when both optimizations are used.  Without lemmas, 19
rise above that threshold.  Without literal merging, 7 do.  Without
either, 25 rise above the threshold.  Two of the benchmarks require
more than $10^8$ proof clauses without lemmas.
\item
  Of the 80 benchmark problems evaluated, and comparing with both
  optimizations disabled, 47 had the number of proof clauses reduced
  by a factor of 2 by using lemmas, 14 by using literal merging, and
  60 by enabling both optimizations.
\end{itemize}

The time performance is less dramatic, but still significant.
Comparing the sum of CPOG generation and checking times for each of
the four combinations of options, we get the following times relative
to the one with both optimizations enabled:
\begin{description}
\item[Without lemmas:] Min 0.89, Max 39.2, Harmonic mean 1.71
\item[Without literal grouping:] Min 0.80, Max 3.77, Harmonic mean 1.11
\item[Without either optimization:] Min 0.96, Max 43.69, Harmonic mean 2.20
\end{description}

Here is another way to summarize the impact on the combined time to generate and validate the CPOG file:
\begin{itemize}
\item Of the 80 benchmark problems evaluated, and comparing with both
  optimizations disabled, 40 had the time reduced by at least a factor of 2
  using lemmas, 6 by using literal merging, and 48 by enabling both
  optimizations.
\end{itemize}

\section{Comparing Monolithic and Structured Validation}

\input{figure-runtime-monolithic}

\input{figure-clauses-monolithic}

Ran all benchmarks for which generation + checking could complete in 1000 seconds.  Total = 83.  All completed in less than 1000 seconds with structured.
Compare both runtimes and total number of clauses for structured vs. monolithic:
\begin{itemize}
\item Structured complete 83.
\item Monolithic completed  70,timed out on 13.
\item Of the 70 completed, 23 were faster with monolithic, 47 were slower (min ratio = 0.013, max ratio = 9.60, hmean = 0.36)
\item Of the 70 completed, 60 had fewer total clauses with monolithic, 10 had more (min ratio = 0.07, max ratio = 1.09, hmean = 0.49)
\end{itemize}

\section{Preprocessing}

\input{figure-runtime-preprocess}
\input{figure-clauses-preprocess}

Use preprocessor in D4.  Enable its three preprocessing methods
(backbone, vivification, occElimination), all of which preserve
equivalence.  Cannot use structured mode for forward implication
proof, since modified clauses do not decompose in the way anticipated.

Consider same 83 benchmarks used to compare monolithic vs. structured mode.
Incorporate time to run D4 into evaluation, since differs according to
how D4 runs.

Runtime results:
\begin{itemize}
\item Monolithic without preprocessing completes 70  of them with an overall time less than 1000 seconds.
\item Monolithic with prepreprocessing completes 69 (the one due to too much time on preprocessing)
\item Of the 69, 53 run faster with preprocessing and 15 run slower (mininum ratio 0.04, maximum ratio 1.65, harmonic mean 0.38)
\end{itemize}

Clause results:
\begin{itemize}
\item Consider the 69 cases that completed for both with overall time less than 1000 seconds.
\item 31 had fewer clauses with preprocessing, 17 were identical, and 21 had more.
\item Min = 0.21, Max = 2.34, Harmonic mean = 0.85
\end{itemize}


\section{Benchmarking the \lean{} Proof Checker}

Ran \lean{} checker on the benchmarks yielding at most $10^7$ clauses, with time limit of 1000 seconds.
\begin{itemize}
\item 80 total
\item All ran to completion
\item All but three ran within factor of $8\times$.  Outliers were small ones
\item Harmonic mean = $5.43\times$.
\end{itemize}

\input{figure-check}


\section{Comparison with MICE}

\input{figure-mice-cpog}


For comparison, we also ran the MICE tool chain for certifying model
counters~\cite{fichte:sat:2022}.  Figure~\ref{fig:mice} compares the
runtimes of our toolchain versus theirs.  The X-axis shows the sum of
the elapsed times for our generator and checker/counter, while the
Y-axis shows the sum of the elapsed times for \textsc{nnf2trace},
generating a file in their trace format from a d-DNNF graph generated
by D4, and for \textsc{sharptrace}, a checker and unweighted counter for the trace format
The plot shows data points for those benchmarks for at least one of
the two tools completed in less than 1000 seconds.

In the figure, red dots indicate cases where neither program had an
error, but it possibly timed out.  The time-out cases are shown along
the upper edge (MICE) or the righthand edge (CPOG).

As can be seen, there is not a strong correlation between the program
runtimes.  This is, in part, because the two tools perform different
roles: one validates the algorithmic steps used by a top-down model
counting problem, while the other verifies the logical equivalence of
a Boolean formula and a representation for which model counting can be
done efficiently.

Overall, we can summarize the results as:
\begin{itemize}
\item 84 total data points
\item 76 were completed by both programs in under 1000 seconds
\item Of those 21 were faster with MICE, 55 with CPOG
\item 7 completed with CPOG under 1000 seconds but exceeded that threshold for MICE
\item 1 completed with MICE under 1000 seconds but exceeded that threshold for CPOG
\end{itemize}


\section{Comparison with CD4 Toolchain}

\input{figure-cd4-cpog}  

Ran D4 with certificate logging enabled (by specifying DRAT file).
Used 114 benchmarks for which regular D4 ran in under 1000 seconds.
Measure time for entire toolchains (compilation, proof generation,
checking).  Checking for CD4 includes running drat-trim.
\begin{itemize}
\item Completed 80 in under 1000 seconds for both
  \begin{itemize}
    \item 7 ran faster with POG framework, up to $2.8\times$; 73 ran slower, up to $119.8\times$.  Harmonic mean = $19.63\times$ slower.
  \end{itemize}
\item 2 completed in under 1000 seconds  with POGs but timed out with CD4.  Both had very few models and with long unsatisfiability proofs
\item 28 completed in under 1000 seconds with CD4 but either timed out or failed with POGs.
\item Remaining 4 exceeded 1000 seconds for both toolchains
\end{itemize}

Found that CD4 cannot handle files that are first preprocessed with D4's built-in preprocessors.

\bibliography{references}


\end{document}



\end{description}

